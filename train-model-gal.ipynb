{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummary in /opt/conda/lib/python3.10/site-packages (1.5.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pydicom in /opt/conda/lib/python3.10/site-packages (2.4.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "! pip install torchsummary\n",
    "! pip install pydicom\n",
    "import torchsummary\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torch.nn as nn\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from concurrent.futures import ProcessPoolExecutor \n",
    "import torch.nn.functional as F\n",
    "import pydicom  \n",
    "from tabulate import tabulate\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f143958ff30>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Config:\n",
    "    # BASE_PATH = '/kaggle/input/rsna-2023-abdominal-trauma-detection'\n",
    "    BASE_PATH = '.'\n",
    "    TRAIN_IMG_PATH='rsna-2023-atd-reduced-256-5mm/reduced_256_tickness_5'\n",
    "    SEED = 42\n",
    "    IMAGE_SIZE = [256, 256]\n",
    "    BATCH_SIZE = 25\n",
    "    EPOCHS = 3\n",
    "    DEPTH=300# median of number of instanceses is 270 ,mean=424\n",
    "    NUM_FOLDS=5\n",
    "    TARGET_COLS = [\n",
    "       'bowel_healthy', 'bowel_injury', 'extravasation_healthy',\n",
    "       'extravasation_injury', 'kidney_healthy', 'kidney_low', 'kidney_high',\n",
    "       'liver_healthy', 'liver_low', 'liver_high', 'spleen_healthy',\n",
    "       'spleen_low', 'spleen_high',\n",
    "    ]\n",
    "\n",
    "torch.manual_seed(Config.SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# addaptation for depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_depth(tensor, target_depth=300):\n",
    "    # This function assumes tensor shape [depth, channel, height, width]\n",
    "    depth = tensor.shape[0]\n",
    "\n",
    "    if depth == target_depth:\n",
    "        return tensor\n",
    "    else:\n",
    "        if tensor.dim() == 3:\n",
    "            tensor = tensor.unsqueeze(0)  # Add a channel dimension\n",
    "        return torch.nn.functional.interpolate(\n",
    "            tensor.unsqueeze(0), \n",
    "            size=(target_depth, tensor.size(2), tensor.size(3)), \n",
    "            mode='trilinear', \n",
    "            align_corners=False\n",
    "        ).squeeze(0).squeeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dicom to png "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dicom_to_img(dicom_image):\n",
    "    dicom_image = pydicom.dcmread(dicom_image)\n",
    "    pixel_array = dicom_image.pixel_array\n",
    "    \n",
    "    if dicom_image.PixelRepresentation == 1:\n",
    "        bit_shift = dicom_image.BitsAllocated - dicom_image.BitsStored\n",
    "        new_array = (pixel_array << bit_shift).astype(pixel_array.dtype) >>  bit_shift\n",
    "        pixel_array = pydicom.pixel_data_handlers.util.apply_modality_lut(new_array, dicom_image)\n",
    "\n",
    "    if dicom_image.PhotometricInterpretation == \"MONOCHROME1\":\n",
    "        pixel_array = 1 - pixel_array\n",
    "\n",
    "    # transform to hounsfield units\n",
    "    pixel_array = pixel_array * dicom_image.RescaleSlope + dicom_image.RescaleIntercept\n",
    "\n",
    "    # windowing\n",
    "    window_center = int(dicom_image.WindowCenter)\n",
    "    window_width = int(dicom_image.WindowWidth)\n",
    "    img_min = window_center - window_width // 2\n",
    "    img_max = window_center + window_width // 2\n",
    "    pixel_array = pixel_array.copy()\n",
    "    pixel_array[pixel_array < img_min] = img_min\n",
    "    pixel_array[pixel_array > img_max] = img_max\n",
    "\n",
    "    # normalization\n",
    "    pixel_array = np.zeros_like(pixel_array) if pixel_array.max() == pixel_array.min() else (pixel_array - pixel_array.min()) / (pixel_array.max() - pixel_array.min())\n",
    "    return pixel_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  utils for image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_labels = pd.read_csv(f\"{Config.BASE_PATH}/train.csv\")\n",
    "#maby drop lower hu \n",
    "\n",
    "#input: entry to tree directory of train images\n",
    "#output:  list of path to scan ( each scan  is a list with path to the images inside it )\n",
    "def reshapePathsToScan(train_img_path):\n",
    "    img_paths = []  \n",
    "    for dirpath, dirnames, filenames in os.walk(train_img_path):\n",
    "        if not filenames:\n",
    "            continue  # skip directories that don't contain files\n",
    "        scan = [os.path.join(dirpath, filename) for filename in filenames]\n",
    "        img_paths.append(scan)\n",
    "    return img_paths\n",
    "def scaleSizeOfDataSet(img_paths,ratio):\n",
    "    max_number_of_images=int(len(img_paths)*ratio)\n",
    "    return img_paths[:max_number_of_images]\n",
    "    \n",
    "def process_image(path):\n",
    "   # Open the image\n",
    "    image = Image.open(path)\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    grayscale = image.convert('L')\n",
    "    \n",
    "    # Convert to numpy array and normalize\n",
    "    grayscale_np = np.array(grayscale) / 255.0\n",
    "    return grayscale_np\n",
    "\n",
    "\n",
    "\n",
    "scansPaths=reshapePathsToScan(Config.TRAIN_IMG_PATH)\n",
    "scansPaths=scaleSizeOfDataSet(scansPaths,1/30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTScansDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CTScansDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, train_labels,scansPaths, current_fold,num_fold=5):\n",
    "        self.train_labels=train_labels\n",
    "        self.scansPaths  = scansPaths \n",
    "        #handle 5 fold validation \n",
    "        self.num_fold = num_fold\n",
    "        self.current_fold = current_fold\n",
    "        self.kfold = KFold(n_splits=num_fold)\n",
    "        self.transform= torchvision.transforms.Compose([\n",
    "                    torchvision.transforms.Resize((256, 256),antialias=True),\n",
    "                    torchvision.transforms.RandomHorizontalFlip(),    # Random horizontal flip\n",
    "                    torchvision.transforms.RandomRotation(45),\n",
    "                    torchvision.transforms.RandomAffine(degrees=0, scale=(0.8, 1.2)),  # Apply zoom transformation\n",
    "                ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.scansPaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #      dicom_images = select_elements_with_spacing(self.img_paths[idx],\n",
    "#                                                     spacing = 2)\n",
    "        #idx to scan-list of images\n",
    "        images_paths=self.scansPaths[idx]\n",
    "        patient_id = images_paths[0].split('/')[-3]\n",
    "        images = [process_image(path) for path in images_paths]\n",
    "        images=np.array(images)\n",
    "        images=self.rescale_scan_to_group_3(images)\n",
    "        # do augmentation to slices- move to 3d becouse we deal with resnet or other-pretraind model.\n",
    "        images = self.transform(torch.from_numpy(images).float())\n",
    "        images=standardize_depth(images)\n",
    "        # get the labels from the df \n",
    "        labels = self.train_labels[self.train_labels.patient_id == int(patient_id)].values[0][1:-1]\n",
    "        return images,labels,patient_id\n",
    "        \n",
    "    def rescale_scan_to_group_3(self,scan):\n",
    "        padding = np.zeros((1,scan.shape[1], scan.shape[2]))\n",
    "        if scan.shape[0]%3 ==1:\n",
    "            scan=np.concatenate((padding,scan,padding),axis=0)\n",
    "        elif  scan.shape[0]%3 ==2:  \n",
    "\n",
    "            scan=np.concatenate((padding,scan),axis=0)\n",
    "        return scan\n",
    "    \n",
    "    def split_fold(self):\n",
    "        \"\"\"\n",
    "        Splits the dataset into training and validation subsets based on the current fold.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: A tuple containing the training and validation subsets.\n",
    "        \"\"\"\n",
    "        #split across patient *-9\n",
    "        fold_data = list(self.kfold.split(self.scansPaths))\n",
    "        train_indices, val_indices = fold_data[self.current_fold]\n",
    "        train_data = Subset(self, train_indices)\n",
    "        val_data = Subset(self,val_indices)\n",
    "        return train_data, val_data\n",
    "    \n",
    "    \n",
    "#define k-fold partition for data -defualt 5 -data loaders\n",
    "\n",
    "folds=[CTScansDataSet(train_labels,scansPaths, current_fold=i).split_fold() for i in range (Config.NUM_FOLDS)]\n",
    "train_folds = [fold[0] for fold in folds]\n",
    "val_folds = [fold[1] for fold in folds]\n",
    "train_dataloaders= [DataLoader(train_folds[i],batch_size = Config.BATCH_SIZE, shuffle = True)  for i in range (Config.NUM_FOLDS) ]\n",
    "val_dataloaders= [DataLoader(val_folds[i],batch_size = Config.BATCH_SIZE, shuffle = False)  for i in range (Config.NUM_FOLDS) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiPathResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiPathResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiPathResNet, self).__init__()\n",
    "\n",
    "#         self.backbone=torchvision.models.mobilenet_v3_large(weights='IMAGENET1K_V1')\n",
    "        self.backbone=torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "#         for param in self.backbone.layer4[-1].parameters():\n",
    "#             param.requires_grad = True \n",
    "        self.backbone = nn.Sequential(*list( self.backbone.children())[:-2])\n",
    "        self.ChannelReducer= nn.Conv2d(in_channels=512, out_channels=128, kernel_size=1)\n",
    "        self.layer_normalization=nn.Sequential(torch.nn.BatchNorm3d(128),nn.ReLU())\n",
    "# avgpool\n",
    "\n",
    "# avgpool\n",
    "# Instantiate the model\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        num_slices=x.size(1)//3\n",
    "        outputs = []\n",
    "        x=x.view(-1, 3, x.size(2), x.size(3))\n",
    "        x=self.backbone(x)\n",
    "        x=self.ChannelReducer(x).view(batch_size,128,num_slices,x.shape[-1],x.shape[-1])\n",
    "        x=self.layer_normalization(x)\n",
    "        return x\n",
    "    \n",
    "class Custom3DCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Custom3DCNN, self).__init__()\n",
    "        # First 3D convolution\n",
    "        \n",
    "        self.convBlock1=nn.Sequential( nn.Conv3d(128, 32, kernel_size=3,\n",
    "                                                stride=(3, 1, 1), padding=(0, 1, 1)),nn.BatchNorm3d(32), nn.ReLU(),nn.Dropout(0.3))\n",
    "        \n",
    "        # Second 3D convolution\n",
    "        self.convBlock2=nn.Sequential( nn.Conv3d(32, 8, kernel_size=3, stride=(3, 1, 1), padding=(0, 1, 1)),\n",
    "                                       nn.BatchNorm3d(8), nn.ReLU(),nn.Dropout(0.3))\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.global_avgpool = nn.AdaptiveAvgPool3d((8, None, None))\n",
    "        self.flatten_size = 8 * 8 * 8 * 8  # Adjusted due to global average pooling\n",
    "        self.Dense=nn.Sequential(nn.Linear(self.flatten_size, 512),nn.BatchNorm1d(512) ,nn.ReLU())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size=x.shape[0]\n",
    "        x=self.convBlock1(x)\n",
    "        x= self.convBlock2(x)\n",
    "        x = self.global_avgpool(x)\n",
    "        x = x.view(batch_size, self.flatten_size)  # Flatten\n",
    "        x =self.Dense(x) \n",
    "        return x\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CT3DModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CT3DModel(nn.Module):\n",
    "    def __init__(self,num_classes=13):\n",
    "        super(CT3DModel, self).__init__()\n",
    "        self.classifier = nn.Linear(512, num_classes)\n",
    "        self.encoder=MultiPathResNet()\n",
    "        self.decoder=Custom3DCNN()\n",
    "    def forward(self, x):\n",
    "            x=self.encoder(x)\n",
    "            x=self.decoder(x)\n",
    "            x = self.classifier(x)\n",
    "            return(x)\n",
    "\n",
    "# print(model)\n",
    "# torchsummary.summary(model,(330 ,256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MetricsCalculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class to calcilate metrics.\n",
    "\n",
    "class MetricsCalculator:\n",
    "    def __init__(self, mode = 'binary'):\n",
    "        \n",
    "        self.probabilities = []\n",
    "        self.predictions = []\n",
    "        self.targets = []\n",
    "        \n",
    "        self.mode = mode\n",
    "        \n",
    "    #logits batch_size*1*n, target batch_size*1*1 \n",
    "    def update(self, logits, target):\n",
    "        \"\"\"\n",
    "        Update the metrics calculator with predicted values and corresponding targets.\n",
    "        \n",
    "        Args:\n",
    "            predicted (torch.Tensor): Predicted values.\n",
    "            target (torch.Tensor): Ground truth targets.\n",
    "        \"\"\"\n",
    "        probabilities = F.softmax(logits, dim = 1)\n",
    "        predicted = torch.argmax(probabilities, dim=1)\n",
    "        if self.mode == 'binary':\n",
    "          #take positive class - for example if has extravision take all probabiltes that patient will have extravation and drop the probabiltes not.\n",
    "          probabilities=probabilities[:,1]\n",
    "            \n",
    "        self.probabilities.extend(probabilities.detach().cpu().numpy())\n",
    "        self.predictions.extend(predicted.detach().cpu().numpy())\n",
    "        self.targets.extend(target.detach().cpu().numpy())\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the stored predictions and targets.\"\"\"\n",
    "        \n",
    "        self.probabilities = []\n",
    "        self.predictions = []\n",
    "        self.targets = []\n",
    "    \n",
    "    def compute_accuracy(self):\n",
    "        \"\"\"\n",
    "        Compute the accuracy metric.\n",
    "        \n",
    "        Returns:\n",
    "            float: Accuracy.\n",
    "        \"\"\"\n",
    "        return accuracy_score(self.targets, self.predictions)\n",
    "    \n",
    "    def compute_auc(self):\n",
    "        \"\"\"\n",
    "        Compute the AUC (Area Under the Curve) metric.\n",
    "        \n",
    "        Returns:\n",
    "            float: AUC.\n",
    "        \"\"\"\n",
    "        if self.mode == 'multi':\n",
    "            return roc_auc_score(self.targets, self.probabilities, multi_class = 'ovo', labels=[0, 1, 2])\n",
    "    \n",
    "        else:\n",
    "            return roc_auc_score(self.targets, self.probabilities)\n",
    "\n",
    "\n",
    "# initialize metrics objects\n",
    "train_acc_bowel = MetricsCalculator('binary')\n",
    "train_acc_extravasation = MetricsCalculator('binary')\n",
    "train_acc_liver = MetricsCalculator('multi')\n",
    "train_acc_kidney = MetricsCalculator('multi')\n",
    "train_acc_spleen = MetricsCalculator('multi')\n",
    "\n",
    "val_acc_bowel = MetricsCalculator('binary')\n",
    "val_acc_extravasation = MetricsCalculator('binary')\n",
    "val_acc_liver = MetricsCalculator('multi')\n",
    "val_acc_kidney = MetricsCalculator('multi')\n",
    "val_acc_spleen = MetricsCalculator('multi')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main_functionmain_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/3]\n",
      "Fold: 0\n",
      "5\n",
      "0\n",
      "tensor([[-3.7712e-02, -6.8837e-01,  9.1414e-02, -2.5361e-01, -3.7734e-01,\n",
      "          2.7402e-01, -1.6277e-01,  2.8522e-01, -7.9483e-01, -1.4469e-01,\n",
      "          3.6017e-01,  5.5910e-01, -1.5474e-01],\n",
      "        [-7.0519e-01,  6.2444e-02, -2.4509e-01, -8.3833e-01,  1.2059e+00,\n",
      "          1.2006e-01, -4.5751e-02,  2.9378e-01, -7.9237e-01,  2.6466e-01,\n",
      "         -9.8815e-02,  6.9820e-01, -1.8758e-01],\n",
      "        [ 9.7030e-03, -1.0763e-01, -3.6144e-01, -2.4421e-01,  5.2821e-02,\n",
      "          4.2737e-01, -2.1990e-01,  5.1859e-02, -2.3752e-01,  6.6336e-01,\n",
      "          8.5738e-01,  1.0944e+00,  2.0961e-01],\n",
      "        [-5.1262e-01,  9.8902e-02, -5.2313e-01, -1.8125e-01,  2.1161e-01,\n",
      "          1.0120e-01, -2.0119e-01,  8.9830e-02,  5.2725e-02, -1.0685e-01,\n",
      "          1.2193e-01,  1.7421e-01,  2.8207e-01],\n",
      "        [ 2.6509e-01, -1.5821e-01, -2.9792e-01, -4.2572e-01,  5.0773e-01,\n",
      "          9.2558e-01, -1.7255e-03,  3.0481e-01, -1.9319e-01, -2.2248e-01,\n",
      "          6.8934e-01,  4.3972e-01,  3.0322e-01],\n",
      "        [-2.7734e-01, -3.1910e-01, -9.7782e-02,  8.0945e-02,  5.3146e-01,\n",
      "          9.0067e-01,  6.5048e-01,  1.7848e-01,  4.4013e-01, -3.3698e-01,\n",
      "         -1.4557e-02,  4.3945e-01,  2.6649e-01],\n",
      "        [ 8.5135e-02, -3.5814e-01, -6.5575e-01, -4.6735e-01, -6.7435e-01,\n",
      "         -9.4295e-02, -1.6754e-01,  4.8794e-01,  5.6579e-01, -6.8282e-02,\n",
      "         -2.8462e-03,  7.6784e-01, -5.9623e-03],\n",
      "        [-3.2294e-01, -2.4856e-01, -3.3336e-01, -3.3715e-01,  4.3731e-03,\n",
      "          7.8470e-02,  4.8499e-01,  1.0231e-01, -5.1581e-02,  2.6051e-01,\n",
      "         -2.1225e-01,  4.5462e-01,  7.4402e-01],\n",
      "        [ 1.2468e-01, -2.8552e-01, -4.0976e-01, -1.0850e-01,  1.0124e-01,\n",
      "          2.9106e-01, -3.8982e-02, -3.2852e-02, -2.9937e-01,  1.0506e-01,\n",
      "         -3.2395e-02,  3.3094e-01,  1.5678e-01],\n",
      "        [ 3.1899e-01,  1.7647e-01,  7.7918e-02, -4.9573e-01,  6.3491e-01,\n",
      "          1.1494e-01, -7.9344e-01, -1.0113e-01, -6.0349e-01,  3.7350e-01,\n",
      "          1.5172e-01,  6.8265e-01, -2.1688e-02],\n",
      "        [-1.1485e-01, -3.4698e-01, -1.1471e-01, -1.2160e-01,  3.6975e-01,\n",
      "          1.0665e-01, -3.3854e-01,  2.5674e-01,  3.5181e-02,  5.5666e-01,\n",
      "          5.2954e-02,  6.2744e-01, -5.0321e-02],\n",
      "        [-1.4920e-01,  4.1902e-02,  2.6232e-01, -4.0638e-01,  6.0665e-01,\n",
      "          1.0340e+00, -1.1887e-01,  1.6573e-03, -5.7568e-03,  2.3694e-01,\n",
      "          1.0963e-01,  7.7434e-01,  1.0852e-01],\n",
      "        [-4.5417e-02,  5.1661e-01, -3.8349e-01,  4.2038e-01, -2.7124e-01,\n",
      "          5.3247e-01, -4.2810e-01,  3.0011e-01,  1.9130e-01, -8.5089e-02,\n",
      "          1.4482e-01,  8.9916e-01,  1.3957e-01],\n",
      "        [ 6.0678e-01, -3.0172e-01,  6.6232e-01,  1.3400e-02,  8.5163e-01,\n",
      "          1.2611e-01, -2.1367e-03,  5.0008e-01,  3.6676e-01,  3.7896e-01,\n",
      "          1.5396e-01,  4.0589e-01,  2.9878e-01],\n",
      "        [-3.2573e-02, -2.4088e-01,  1.8517e-01,  1.7225e-01, -2.9743e-03,\n",
      "          1.9764e-01,  3.9366e-01,  7.4086e-02, -1.3935e-01, -2.8270e-01,\n",
      "          2.3697e-01,  4.4756e-01, -1.4541e-01],\n",
      "        [-8.3152e-02, -3.0203e-01, -1.5860e-01, -6.4257e-02, -2.0959e-01,\n",
      "          3.5420e-01, -2.5131e-02,  4.9226e-01, -7.6237e-02, -3.8484e-01,\n",
      "          2.5608e-01,  5.5322e-01,  1.1301e-01],\n",
      "        [-2.6893e-01, -5.9062e-01, -6.9753e-02, -7.0226e-02, -5.3461e-01,\n",
      "          7.9857e-01,  3.0215e-01,  4.1670e-01, -2.9598e-01, -1.4862e-01,\n",
      "         -3.1087e-01,  3.5188e-01,  2.3859e-01],\n",
      "        [ 1.1505e-01, -4.3165e-03, -2.0976e-01, -5.1504e-01,  1.4208e-01,\n",
      "          1.8811e-01,  6.0483e-01,  2.2303e-01,  3.4134e-01,  2.6560e-01,\n",
      "         -3.8469e-01,  4.9840e-01, -7.1730e-02],\n",
      "        [ 1.2712e-01,  2.8299e-01,  5.2703e-01, -1.0186e-01, -2.0161e-01,\n",
      "          1.2461e-01, -5.7011e-02, -1.0344e-01,  3.2755e-01,  1.1490e-01,\n",
      "          7.2322e-01,  5.3438e-01,  6.0267e-01],\n",
      "        [ 2.4538e-02,  2.3730e-01, -2.4158e-02,  1.8804e-01,  2.8597e-01,\n",
      "         -1.7240e-02, -1.6764e-02,  1.7511e-01, -2.8368e-01, -6.7497e-01,\n",
      "         -1.4433e-02,  5.6678e-01, -1.0966e-01],\n",
      "        [ 1.1248e-01,  3.4284e-01, -7.5613e-01, -3.8538e-01,  3.2128e-01,\n",
      "          6.2733e-01,  4.0307e-01,  2.3794e-01, -2.7790e-01, -5.3616e-01,\n",
      "          2.7066e-01,  6.1029e-01,  6.5578e-01],\n",
      "        [ 2.8190e-01,  1.1778e-01, -9.9531e-01, -1.5859e-01,  3.6009e-02,\n",
      "          2.5934e-01, -4.1579e-01,  8.9092e-01, -9.1302e-02,  2.9929e-01,\n",
      "          3.2866e-01,  6.4508e-01,  3.9501e-01],\n",
      "        [-4.0107e-01,  5.6941e-04,  1.7439e-01, -3.6376e-01, -4.1140e-01,\n",
      "          9.5183e-01, -2.0275e-01,  5.3980e-01, -2.3874e-02,  3.8825e-02,\n",
      "          5.7946e-01,  7.4920e-01,  5.6847e-02],\n",
      "        [ 6.5916e-02, -5.6205e-01, -2.8955e-01, -5.5497e-01,  6.0206e-01,\n",
      "          3.7931e-01, -2.8033e-01, -9.6954e-02, -6.5500e-01,  5.1837e-01,\n",
      "         -3.2555e-01,  8.2489e-01, -2.0209e-01],\n",
      "        [ 5.6906e-01,  2.0624e-01, -4.5390e-01, -4.3492e-01,  4.2373e-01,\n",
      "         -3.7178e-01,  2.1627e-01,  6.2472e-01,  4.5543e-01,  1.3748e-01,\n",
      "         -1.5942e-01,  1.0779e+00,  4.2065e-01]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "torch.float32\n",
      "tensor(4.9738, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "5\n",
      "1\n",
      "tensor([[-0.2516,  0.1819,  0.1243, -0.0263,  0.6350,  0.3192, -0.3186, -0.0463,\n",
      "         -0.2004,  0.0890,  0.3136,  0.9661, -0.2740],\n",
      "        [-0.4226, -0.5061, -0.4716, -0.9831,  0.1787,  0.2179, -0.5095,  0.3721,\n",
      "          0.0582,  0.0601,  0.5704,  0.2913,  0.0547],\n",
      "        [ 0.4374,  0.2666,  0.0170,  0.1450, -0.2503,  0.4694,  0.7247,  0.6515,\n",
      "         -0.1030,  0.5722, -0.2364,  0.6970,  0.4746],\n",
      "        [ 0.1961,  0.1741, -0.6530, -0.3494,  0.2549,  0.3601,  0.0296,  0.1474,\n",
      "         -0.1139,  0.4615,  0.2779,  0.6718,  0.0829],\n",
      "        [ 0.1076, -0.1490, -0.1770, -0.0777,  0.4015,  0.4070,  0.0411,  0.6042,\n",
      "          0.4282, -0.6828,  0.0970,  0.9647,  0.0454],\n",
      "        [-0.4319, -0.2015,  0.2528, -0.2915,  0.2176,  0.3301, -0.1369, -0.0670,\n",
      "          0.0280,  0.2223,  0.2855,  0.2053,  0.0185],\n",
      "        [ 1.0891, -0.2322,  0.4606, -0.1101,  0.6168,  0.5581,  0.3391,  0.3828,\n",
      "          0.8477,  0.1283,  0.2190,  0.6794, -0.0074],\n",
      "        [ 0.1921, -0.2610, -0.0086, -0.5596,  0.7261,  0.1294, -0.1465,  0.5662,\n",
      "         -0.0880, -0.1712,  0.3132,  0.7112, -0.0688],\n",
      "        [ 0.1255, -0.0364,  0.0521, -0.1765,  0.0042,  0.7780,  0.2191,  0.0375,\n",
      "         -0.2349, -0.3089,  0.1592,  0.1821,  0.4610],\n",
      "        [-0.3004, -0.3751, -1.2119, -1.4092,  0.3953,  0.4402, -0.0806,  0.3775,\n",
      "         -0.3956,  0.7073, -0.2533,  0.8277,  0.6505],\n",
      "        [-0.3169, -0.8673, -0.6622,  0.1872,  0.5556,  0.4407,  0.0237,  0.0221,\n",
      "         -0.0915, -0.3245,  0.0916,  0.3956,  0.1813],\n",
      "        [-0.3723, -0.0198, -0.0469, -0.1915,  0.3802,  0.0161, -0.4262,  0.2714,\n",
      "         -0.2418, -0.0678, -0.1862,  1.3014,  0.3731],\n",
      "        [-0.6245,  0.2303, -0.3800, -0.2719,  0.2048,  0.3316, -0.3391,  0.5928,\n",
      "         -0.2498,  0.3639,  0.8324,  1.0502,  0.5551],\n",
      "        [ 0.6672, -0.0962,  0.6171, -0.1787, -0.0964,  0.1005, -0.1677,  0.7884,\n",
      "          0.1474,  0.2071,  0.2483,  0.9611,  0.1791],\n",
      "        [ 0.1884,  0.0257, -0.3698, -0.5676,  0.0767,  0.1787,  0.0017, -0.0676,\n",
      "          0.0519, -0.0516,  0.0942,  0.5581,  0.8835],\n",
      "        [ 0.8001,  0.0832, -0.0514, -0.2393,  0.3288,  0.2512,  0.3079, -0.1959,\n",
      "         -0.4719, -0.2221, -0.0955,  0.4116, -0.0974],\n",
      "        [ 0.1908,  0.5978, -0.2520, -0.0779,  0.0843,  0.2822, -0.3237, -0.1030,\n",
      "         -0.0167, -0.0348,  0.0225,  0.3655,  0.1318],\n",
      "        [ 0.0586,  0.4691, -0.5365, -0.1226,  0.6162,  0.5122, -0.3021,  0.5701,\n",
      "         -0.0020,  0.1519,  0.7751,  0.7278, -0.2262],\n",
      "        [-0.2862, -0.1752, -0.1593,  0.1365,  0.2724,  0.3021,  0.1699,  0.4445,\n",
      "         -0.0541, -0.0512,  0.1501,  0.3260,  0.2384],\n",
      "        [ 0.0539, -0.1307, -0.3257, -0.3484,  0.1570,  0.4023, -0.2099,  0.0686,\n",
      "         -0.2567, -0.1907, -0.0190,  0.0250,  0.4972],\n",
      "        [-0.2095, -0.3089, -0.1709, -0.6575,  0.3292,  0.3949, -0.5062,  0.2779,\n",
      "         -0.4419,  0.2535, -0.0399,  0.5540, -0.1999],\n",
      "        [-0.1477, -0.4370, -0.5121,  0.5443, -0.4399,  0.0311,  0.4116,  0.4710,\n",
      "         -0.5152, -0.5549,  0.0219,  0.7415,  0.1294],\n",
      "        [-0.0041,  0.1888,  0.2557, -0.2504, -0.1121,  0.3651,  0.2173,  0.1362,\n",
      "          0.2581,  0.0711,  0.3484,  0.1111,  0.3917],\n",
      "        [ 0.2026, -0.5694, -0.0554,  0.1104, -0.5891,  0.5190, -0.1106,  0.4583,\n",
      "         -0.0169,  0.3385,  0.3035,  0.6168, -0.3003],\n",
      "        [-0.1659, -0.7014,  0.0493,  0.4092, -0.0361, -0.2649, -0.3470,  0.2579,\n",
      "         -0.5377, -0.3956, -0.0984,  0.5706, -0.0772]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "torch.float32\n",
      "tensor(4.8884, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "5\n",
      "2\n",
      "tensor([[-5.8715e-02, -1.7809e-01,  2.4917e-02, -2.3669e-01,  6.9162e-01,\n",
      "          6.9074e-01, -6.1852e-02,  2.5249e-01, -3.1752e-02,  1.8599e-01,\n",
      "          6.4966e-02, -9.8814e-02,  5.1021e-02],\n",
      "        [ 2.1804e-01,  3.4195e-01, -5.2738e-01, -3.6205e-01,  2.1333e-01,\n",
      "          1.6365e-01, -2.5702e-02,  1.9927e-01,  1.0065e-01, -2.2353e-01,\n",
      "          4.1236e-01,  1.2215e-01,  6.3453e-01],\n",
      "        [ 7.7297e-02, -4.1223e-01, -6.2655e-02, -2.0371e-01,  1.5487e-01,\n",
      "          8.5322e-01, -1.3994e-01,  5.1006e-01, -3.6342e-01,  3.3299e-01,\n",
      "         -1.7049e-01,  6.2863e-01,  6.2185e-01],\n",
      "        [ 2.3305e-01, -3.1182e-01,  2.1443e-01, -5.5345e-01,  4.0915e-01,\n",
      "          3.6073e-01, -1.4590e-01,  3.8474e-02, -1.1672e-01,  1.6747e-01,\n",
      "          4.5310e-01,  2.1917e-01, -5.1798e-03],\n",
      "        [ 2.5064e-01, -7.0273e-01, -2.7568e-01,  6.1153e-02, -3.9145e-02,\n",
      "          8.2274e-01, -1.4557e-01,  7.7158e-01, -4.3767e-01, -7.2389e-02,\n",
      "         -1.9643e-01,  1.0299e+00, -2.9291e-02],\n",
      "        [-3.7174e-01,  2.2949e-01,  1.4048e-01, -2.8388e-01, -2.8145e-02,\n",
      "         -6.6918e-02, -6.2353e-02,  6.3340e-01, -5.4252e-01,  3.6559e-02,\n",
      "          5.8095e-02,  1.9884e-01,  7.2859e-01],\n",
      "        [ 5.1579e-01,  1.5113e-01, -9.8832e-01,  5.2095e-02,  7.0836e-02,\n",
      "          1.5306e-01,  3.6574e-01,  3.9330e-01, -5.7042e-01,  1.4701e-01,\n",
      "         -1.5049e-01,  1.2381e-01,  4.0622e-01],\n",
      "        [ 7.8422e-03, -2.7070e-02, -4.4110e-01, -5.3138e-01,  3.2545e-01,\n",
      "          2.6978e-01, -7.4587e-01,  5.9670e-01,  2.2880e-01,  3.0398e-01,\n",
      "          1.0147e+00,  5.6171e-01,  9.9843e-02],\n",
      "        [ 1.2420e-01,  8.0381e-01, -6.3739e-01, -1.0373e+00,  2.7439e-01,\n",
      "          1.8169e-03, -1.2280e-02,  2.6821e-01, -2.6757e-01,  2.0625e-01,\n",
      "          6.3456e-02,  6.5762e-01,  4.8518e-01],\n",
      "        [ 1.4852e-01, -3.6972e-01,  3.7008e-01, -3.5272e-01, -6.1767e-01,\n",
      "          2.4519e-01, -8.9860e-02,  8.4311e-01, -5.2551e-01, -3.4008e-01,\n",
      "          3.4837e-01,  3.0731e-01, -2.1197e-01],\n",
      "        [-1.6435e-02, -2.8545e-01,  1.1087e-01, -5.3089e-01,  3.9150e-01,\n",
      "          8.2275e-01, -1.4158e-01,  7.8177e-01,  3.3768e-01, -4.9989e-02,\n",
      "         -8.0015e-03,  1.1064e+00, -4.1144e-01],\n",
      "        [ 7.4924e-02, -3.0653e-01, -8.4895e-01, -2.9662e-01,  5.6098e-02,\n",
      "          7.1464e-01, -2.5904e-01,  2.4995e-01, -8.0890e-02,  5.5385e-01,\n",
      "          1.2421e-01,  2.7477e-01, -3.0776e-01],\n",
      "        [-5.1015e-02, -8.5196e-02,  1.2448e-02,  3.4484e-01,  6.6807e-01,\n",
      "          2.3455e-01,  5.0413e-01,  1.6752e-02,  5.4352e-01, -1.4859e-01,\n",
      "          4.7127e-01,  9.4038e-01, -1.1986e-02],\n",
      "        [-5.9007e-01, -7.5604e-01,  1.6503e-02, -2.1300e-02,  7.2537e-01,\n",
      "          2.4523e-01,  5.4162e-01,  7.8873e-01,  5.9088e-02,  6.8200e-02,\n",
      "          1.0479e-01,  4.0388e-01,  5.5857e-01],\n",
      "        [ 7.4671e-02,  1.9749e-01, -1.5764e-01, -1.5711e-01,  1.5669e-01,\n",
      "          2.9000e-01,  5.5604e-01,  2.4850e-01, -4.2083e-01,  6.0536e-01,\n",
      "          1.4365e-02,  3.5103e-01,  1.2662e-02],\n",
      "        [-1.3556e-03,  9.4772e-02, -4.8505e-02, -1.5614e-01, -1.6175e-01,\n",
      "         -2.1224e-02, -2.2977e-01, -1.2874e-02, -3.1986e-01,  2.6781e-01,\n",
      "         -9.3078e-02,  4.6899e-01,  2.8262e-01],\n",
      "        [-3.0676e-02, -3.4194e-01, -1.9820e-01, -1.6853e-01,  2.3741e-01,\n",
      "          2.7219e-01, -2.4801e-01,  4.8337e-01, -8.8917e-01, -1.2435e-01,\n",
      "         -7.6833e-02,  1.1283e+00,  3.7290e-01],\n",
      "        [-3.1837e-01,  7.6387e-02, -3.3513e-01, -2.9386e-01,  2.5627e-02,\n",
      "          1.5204e-01,  4.2583e-01,  3.2281e-02,  2.9422e-01, -1.0021e-01,\n",
      "          1.3908e-01,  3.1472e-01,  1.9199e-01],\n",
      "        [ 1.0893e-01, -5.0774e-01, -2.8971e-01, -7.8803e-01,  4.0685e-02,\n",
      "          2.7057e-01, -4.7587e-01,  3.4140e-01, -2.9631e-02, -2.9642e-01,\n",
      "          7.5634e-01,  7.2522e-01,  2.5225e-01],\n",
      "        [ 7.7558e-02,  1.9479e-01, -6.1976e-01, -2.6070e-01,  2.8357e-01,\n",
      "          3.1508e-01, -4.9738e-01,  1.4575e-01, -1.2112e-01,  3.5636e-01,\n",
      "          2.8077e-01,  6.7079e-01,  1.5349e-01],\n",
      "        [ 6.4043e-01, -6.6562e-02, -8.9587e-02, -3.7160e-01, -3.5143e-01,\n",
      "         -2.2073e-01,  5.5699e-01, -1.4352e-01,  2.3400e-01, -8.5399e-02,\n",
      "          4.6355e-01,  7.4329e-01,  1.4942e-01],\n",
      "        [ 1.9669e-02, -7.4058e-01, -4.4247e-02, -2.7749e-01,  7.8732e-01,\n",
      "          2.9490e-02, -8.5359e-01,  8.5540e-03, -1.1392e-01, -9.9836e-01,\n",
      "         -3.3792e-01,  4.3320e-01,  3.0976e-01],\n",
      "        [-7.1442e-02, -2.1585e-01, -6.6789e-01, -1.9154e-01,  3.4395e-01,\n",
      "          1.8542e-01, -7.0117e-01, -3.1938e-01,  1.1471e-01, -1.3559e-01,\n",
      "          4.4617e-01,  9.5168e-01, -1.6178e-01],\n",
      "        [ 4.5835e-01,  3.0724e-01, -8.9101e-02,  7.3625e-02,  4.2137e-01,\n",
      "         -1.5281e-01,  1.9765e-02, -1.7606e-01, -1.8101e-01,  1.1139e-01,\n",
      "          2.7316e-01,  1.7338e+00,  3.0909e-01],\n",
      "        [-1.7150e-01, -2.6140e-01,  1.6936e-03,  9.5804e-01,  8.3376e-01,\n",
      "          7.0082e-01,  5.7950e-01,  1.8463e-01,  3.9459e-01, -1.1775e-01,\n",
      "          9.7127e-02,  5.9517e-01, -2.6122e-01]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "torch.float32\n",
      "tensor(4.8009, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "5\n",
      "3\n",
      "tensor([[ 0.1991,  0.2239,  0.1202, -0.5080,  0.0208,  0.4160, -0.5594,  0.1501,\n",
      "         -0.7026,  0.7386,  0.3593,  0.6701,  0.5934],\n",
      "        [-0.1090, -0.2846, -0.2404,  0.0399,  0.0636,  0.6049,  0.0527,  0.4136,\n",
      "         -0.0260, -0.0287,  0.1406,  0.4231,  0.0863],\n",
      "        [ 0.0015, -0.2060, -0.3819, -0.1388,  0.2226,  0.1467, -0.3244,  0.0213,\n",
      "         -0.5048,  0.3674,  0.6028,  0.2571,  0.1171],\n",
      "        [ 0.5526, -0.3289, -0.8934, -0.0117, -0.5600,  0.4931, -0.4482,  0.4777,\n",
      "          0.2414, -0.1362,  0.0628,  0.3761, -0.0840],\n",
      "        [-0.1821, -0.2596,  0.2365, -0.0057,  0.2883, -0.3997,  0.0161, -0.1618,\n",
      "         -0.2550,  0.1390,  0.7411,  0.2520,  0.3552],\n",
      "        [ 0.1994,  0.0747, -0.3351, -0.1485, -0.0047,  0.7594, -0.4813,  0.5505,\n",
      "          0.1470,  0.1352,  0.3612,  0.3180,  0.1726],\n",
      "        [-0.0759,  0.2051, -0.0767, -0.4404,  0.5940,  0.3288,  0.0805,  0.2041,\n",
      "         -0.4477, -0.0968, -0.0851,  0.0690, -0.3496],\n",
      "        [-0.0962, -0.4062,  0.2064,  0.1619, -0.3032,  0.3598, -0.2332,  0.4996,\n",
      "         -0.1958, -0.5130, -0.2775,  0.3992,  0.0631],\n",
      "        [ 0.1427,  0.0107,  0.4194,  0.0752,  0.4649,  0.3805,  0.3537,  0.3600,\n",
      "         -0.3070,  0.1301,  0.4886,  0.2619,  0.8693],\n",
      "        [ 0.8303, -0.0166,  0.3573, -0.2065, -0.1833,  0.4836,  0.0436,  0.7296,\n",
      "          0.1293,  0.2061, -0.1727,  0.6655,  0.4447],\n",
      "        [-0.1768, -0.6963,  0.4353,  0.4032, -0.1801,  0.9003, -0.3837,  0.3055,\n",
      "         -0.2797, -0.4536, -0.0481,  1.0558,  0.2531],\n",
      "        [ 0.3708,  0.1587, -0.0878, -0.6813,  0.2760, -0.0204,  0.0878,  0.4334,\n",
      "          0.1663, -0.2370,  0.2541,  0.4241, -0.1487],\n",
      "        [-0.0179, -0.3925,  0.0839, -0.0800, -0.4838, -0.1014,  0.0262,  0.9431,\n",
      "         -0.2849, -0.1890,  0.6948,  1.0567,  0.5797],\n",
      "        [-0.0300, -0.2917, -0.7604, -0.6757, -0.0570,  0.7783, -0.3119,  0.6988,\n",
      "          0.4511, -0.1019,  0.5296,  1.2068, -0.7117],\n",
      "        [-0.3552, -0.0579, -0.5591, -0.0894,  0.4002, -0.1939, -0.2408,  0.5815,\n",
      "          0.3136,  0.5921,  0.5371,  0.3200,  0.4307],\n",
      "        [-0.0297,  0.0798, -0.6256,  0.3347,  0.3111, -0.4234,  0.0187, -0.4054,\n",
      "         -0.0596, -0.2044, -0.2399,  0.3958, -0.0847],\n",
      "        [ 0.0446,  0.3342,  0.0712,  0.1952,  0.8737,  0.7307, -0.2337,  0.6764,\n",
      "         -0.4358, -0.5436,  0.0655,  0.9843, -0.3647],\n",
      "        [ 0.0431, -0.0591, -0.0880, -0.6093,  0.4368,  0.7125,  0.4467,  0.1662,\n",
      "          0.5140, -0.1353, -0.1590,  1.3741, -0.1030],\n",
      "        [ 0.6701, -0.7000, -0.4666,  0.0987, -0.1484,  0.5274, -0.2484,  0.1672,\n",
      "         -0.2694,  0.2716,  0.1938,  0.5704,  0.5652],\n",
      "        [ 0.1775, -0.0661,  0.1499, -0.4217,  1.0612, -0.0209, -0.1889,  0.7589,\n",
      "         -0.0319,  0.3462,  0.3930,  0.9733, -0.0476],\n",
      "        [ 0.6516, -0.1647, -0.1841, -0.7173,  0.5178,  0.6990,  0.3724,  0.7050,\n",
      "          0.0410,  0.0064,  0.1500,  0.9077,  0.0073],\n",
      "        [-0.3199, -0.7577, -0.5099, -0.4878,  0.3924, -0.1970, -0.5727, -0.1885,\n",
      "         -0.5594, -0.3728,  0.5602, -0.2511, -0.3140],\n",
      "        [ 0.2009, -0.2264, -0.4010, -0.5111,  0.5843,  0.4514, -0.4997, -0.1894,\n",
      "         -0.8700,  0.2861, -0.1961,  0.6248,  0.5742],\n",
      "        [-0.4192, -0.0612, -0.2209, -0.4279,  0.3680, -0.3536,  0.1641, -0.1063,\n",
      "          0.3713, -0.2001,  0.3964,  0.3352,  0.2004],\n",
      "        [-0.1159, -0.1368, -0.6681, -0.8627,  0.4505,  0.2347,  0.4353, -0.0421,\n",
      "         -0.3784, -0.0883, -0.1472,  0.4098,  0.4625]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "torch.float32\n",
      "tensor(4.5584, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "5\n",
      "4\n",
      "tensor([[ 7.8548e-01, -3.5216e-01, -3.7058e-01, -2.1501e-01,  1.9422e-01,\n",
      "          8.5054e-01, -4.6749e-01,  5.2811e-01, -2.1344e-01, -2.9333e-01,\n",
      "          5.1152e-01,  2.1324e-01,  1.4712e-01],\n",
      "        [ 1.3617e-01,  6.9709e-03, -7.0519e-01, -4.2550e-01, -2.9711e-02,\n",
      "          6.9942e-02, -4.2366e-01,  1.4692e-02, -3.6042e-01, -6.1300e-02,\n",
      "          2.4810e-02,  1.3066e+00,  2.4928e-01],\n",
      "        [ 6.1965e-01,  1.8727e-01, -1.1137e-01, -1.1600e-01,  3.8439e-01,\n",
      "          7.0448e-01, -5.7112e-01,  2.8044e-01, -5.0711e-01, -5.1395e-01,\n",
      "          8.0626e-01,  5.4667e-01, -1.1224e-01],\n",
      "        [-5.0345e-01, -5.0352e-01, -1.6791e-01, -4.4063e-01, -4.4793e-01,\n",
      "          3.5412e-01,  3.6821e-02, -2.5530e-02,  1.4622e-01, -4.8982e-01,\n",
      "         -5.7217e-02,  8.1102e-01,  1.6724e-01],\n",
      "        [ 5.3354e-01, -2.9528e-01, -1.0314e-01, -3.9919e-01,  9.6160e-02,\n",
      "          4.9057e-01,  1.4887e-01,  4.2028e-01, -1.9954e-01, -2.9025e-01,\n",
      "          5.4016e-02,  6.5267e-01,  2.2170e-01],\n",
      "        [ 5.7698e-01,  9.1825e-02, -2.2796e-01, -3.7196e-01,  5.9902e-01,\n",
      "          1.5258e-02,  7.9671e-02,  6.6840e-01, -2.1469e-01,  3.1083e-01,\n",
      "          1.1526e-01,  1.2295e-01,  7.3162e-02],\n",
      "        [ 2.7807e-02,  4.0893e-01, -3.7431e-01, -5.1242e-01, -5.3797e-02,\n",
      "          6.9256e-01, -1.3171e-01, -1.7006e-01, -2.2677e-01,  5.5017e-02,\n",
      "         -1.8431e-02,  7.0859e-01,  2.1128e-02],\n",
      "        [ 2.2215e-01, -3.2795e-01,  1.0848e-01, -3.8867e-01,  5.9126e-01,\n",
      "          7.6453e-01,  3.9023e-01,  5.7753e-01, -2.3373e-03,  1.7962e-01,\n",
      "          5.5273e-01,  1.1627e+00,  6.1205e-02],\n",
      "        [-4.0196e-01, -2.8729e-01,  1.1254e-01, -1.9358e-01,  6.3903e-01,\n",
      "          2.9493e-01,  3.7904e-01, -3.8382e-01,  1.4268e-01,  5.8425e-01,\n",
      "          4.3195e-01,  3.3285e-01, -6.4397e-02],\n",
      "        [-3.4019e-01, -2.0245e-01, -1.3669e-01, -6.4539e-01,  1.0461e-01,\n",
      "          5.8586e-01, -4.6295e-01,  3.5813e-01,  2.9382e-01,  2.8542e-01,\n",
      "          1.1710e-03,  1.0045e-01,  6.9144e-01],\n",
      "        [ 4.1999e-02,  2.1809e-01,  1.7772e-01, -2.6639e-01, -9.8463e-02,\n",
      "          5.1433e-01,  1.6388e-01,  6.8783e-01,  3.3839e-01,  5.7035e-01,\n",
      "          2.5206e-01,  3.3124e-01,  5.9294e-01],\n",
      "        [ 1.2117e-01, -2.2117e-01, -4.6390e-01, -1.9165e-01,  2.2102e-01,\n",
      "          6.0379e-01, -2.4884e-01,  4.1794e-01, -8.4153e-02, -1.9341e-01,\n",
      "         -1.3545e-01,  1.6576e-01,  1.1511e-01],\n",
      "        [ 2.3664e-01, -1.5974e-01,  1.3699e-01, -1.4216e-01,  3.9312e-01,\n",
      "          9.2930e-01,  1.9107e-01,  1.0565e+00, -4.5980e-01, -9.0524e-02,\n",
      "          3.5348e-01,  5.7886e-01, -1.9879e-02],\n",
      "        [ 3.9881e-01, -5.0683e-01,  4.4820e-01, -1.7012e-01,  8.8480e-01,\n",
      "          1.4011e-01, -1.8683e-01,  3.5641e-01, -6.2882e-01,  1.8705e-01,\n",
      "          2.9103e-01,  3.0560e-01,  4.6516e-01],\n",
      "        [-1.0237e+00, -1.5111e-01, -7.1425e-01, -2.0911e-01,  5.1065e-01,\n",
      "         -6.8438e-02,  5.3366e-01, -4.7065e-02, -1.4269e-01,  5.3777e-02,\n",
      "          7.6376e-01,  6.3033e-01, -1.3878e-01],\n",
      "        [ 1.6019e-01,  2.7063e-01, -7.1929e-02, -5.6621e-01,  2.4861e-01,\n",
      "         -1.8514e-01,  5.9002e-01,  3.6930e-01,  1.6351e-01,  2.4885e-01,\n",
      "         -2.7840e-01,  4.2294e-01,  1.8961e-01],\n",
      "        [-6.2389e-02, -3.3252e-01,  2.4054e-01, -2.0802e-01,  4.7827e-01,\n",
      "          6.0263e-02,  5.3655e-01,  6.3862e-01,  1.5416e-01,  1.4710e-02,\n",
      "          8.1080e-02,  8.3663e-01,  1.8500e-01],\n",
      "        [-3.3718e-01,  3.8855e-02, -2.9101e-01, -7.5353e-02,  7.7803e-02,\n",
      "         -2.2396e-01, -4.9302e-01,  4.0675e-01, -1.9425e-02, -4.3011e-01,\n",
      "          1.7971e-01,  3.9255e-01, -5.4485e-02],\n",
      "        [ 5.3806e-01, -1.3040e-01,  2.2306e-02, -2.6406e-02, -2.7451e-04,\n",
      "          3.3984e-01, -9.6265e-01,  5.1913e-02, -2.2314e-01, -1.9619e-01,\n",
      "          4.9539e-01,  5.1705e-01, -6.7115e-02],\n",
      "        [ 1.9502e-01,  5.3915e-02, -3.5056e-01, -3.1893e-01,  1.6365e-01,\n",
      "         -3.0323e-01, -3.8011e-01,  2.6552e-01, -6.5403e-01, -4.5761e-02,\n",
      "          1.7632e-01,  6.6142e-01,  4.9373e-01],\n",
      "        [ 3.1517e-02, -4.7935e-01, -6.2437e-01, -6.2991e-02,  2.6911e-03,\n",
      "          6.4855e-01, -5.5433e-01, -4.5860e-02, -5.9612e-01, -9.3665e-02,\n",
      "          3.4593e-01,  2.1695e-01, -1.3725e-01],\n",
      "        [-3.2953e-01, -2.9119e-01, -4.3694e-01,  1.6377e-01, -1.5632e-01,\n",
      "          6.9333e-02,  8.1683e-02,  1.2220e-01, -7.7339e-02, -1.4018e-01,\n",
      "          1.7340e-01,  1.0107e+00,  2.7008e-01],\n",
      "        [ 2.2828e-01, -3.5166e-01, -3.1817e-01, -3.5146e-01,  6.4721e-01,\n",
      "         -2.0828e-01, -3.7478e-01,  2.2538e-01,  4.3066e-03,  7.8067e-02,\n",
      "          3.0434e-01,  9.2676e-01,  1.1273e-01],\n",
      "        [ 5.3348e-01, -1.2449e-01,  1.7894e-01,  2.3318e-01,  4.4628e-01,\n",
      "         -2.1145e-01,  1.0050e-01,  5.8850e-01, -4.9498e-01, -4.0773e-01,\n",
      "          2.5956e-01,  1.8982e-01, -8.1042e-02],\n",
      "        [-1.0504e-01, -3.4503e-01,  1.3522e-01,  6.3425e-02,  6.8201e-01,\n",
      "         -3.1702e-02, -7.0517e-01,  1.7653e-01,  1.3228e-02, -4.0946e-01,\n",
      "          8.6584e-02,  5.0671e-01,  1.6782e-01]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "torch.float32\n",
      "tensor(4.7095, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch [1/3] - train Loss: 4.7862 - Val Loss: 4.8072\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0]\n",
      "[0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0.34284055, 0.6830079, 0.47070137, 0.6482872, 0.39572746, 0.48956168, 0.3909611, 0.51858675, 0.39886454, 0.46443042, 0.4422266, 0.5476316, 0.6369218, 0.28730723, 0.44811103, 0.44549814, 0.42026585, 0.47019345, 0.5388897, 0.5529914, 0.55733556, 0.4590599, 0.59908205, 0.34797263, 0.41027564, 0.6066907, 0.47912544, 0.45741385, 0.4945084, 0.43620154, 0.55735904, 0.21061167, 0.3886242, 0.45961392, 0.48131993, 0.3657861, 0.5872272, 0.7015847, 0.31791344, 0.4594029, 0.32807928, 0.6003635, 0.60121584, 0.5277406, 0.45399454, 0.47515273, 0.42816547, 0.5480674, 0.3160401, 0.36922467, 0.4701928, 0.5309386, 0.38000467, 0.36705497, 0.27820793, 0.64593816, 0.40983218, 0.49127284, 0.66365296, 0.37326255, 0.433149, 0.40577653, 0.4914556, 0.45860374, 0.53066564, 0.5240134, 0.42280704, 0.5974275, 0.35053897, 0.5292742, 0.33026397, 0.31859237, 0.4639616, 0.46229213, 0.47754005, 0.5061814, 0.45620057, 0.4483069, 0.2928555, 0.4806311, 0.46887323, 0.5697794, 0.42310777, 0.46706232, 0.3000974, 0.3729733, 0.44719708, 0.4074351, 0.43493792, 0.5737903, 0.52734864, 0.57189685, 0.47448924, 0.20259374, 0.4394144, 0.30654323, 0.39227757, 0.39477554, 0.5885451, 0.49478012, 0.24275278, 0.46774566, 0.3935593, 0.49998277, 0.3038939, 0.3810357, 0.5941444, 0.3658406, 0.5286363, 0.53438234, 0.54390967, 0.41524044, 0.40218145, 0.28789222, 0.7052733, 0.52758354, 0.43287483, 0.59291714, 0.33884263, 0.4647816, 0.3749893, 0.5095819, 0.35894763, 0.34119475, 0.4402886]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/src/train-model-gal.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bsshd.jarvislabs.ai/home/src/train-model-gal.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=178'>179</a>\u001b[0m                     plt\u001b[39m.\u001b[39maxis(\u001b[39m'\u001b[39m\u001b[39moff\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# Turn off axis labels\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bsshd.jarvislabs.ai/home/src/train-model-gal.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=180'>181</a>\u001b[0m                 plt\u001b[39m.\u001b[39mshow()\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bsshd.jarvislabs.ai/home/src/train-model-gal.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=182'>183</a>\u001b[0m main_function()\n",
      "\u001b[1;32m/home/src/train-model-gal.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bsshd.jarvislabs.ai/home/src/train-model-gal.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=113'>114</a>\u001b[0m \u001b[39mprint\u001b[39m(train_acc_bowel\u001b[39m.\u001b[39mpredictions)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bsshd.jarvislabs.ai/home/src/train-model-gal.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=114'>115</a>\u001b[0m \u001b[39mprint\u001b[39m(train_acc_bowel\u001b[39m.\u001b[39mprobabilities)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bsshd.jarvislabs.ai/home/src/train-model-gal.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=115'>116</a>\u001b[0m metrics_data \u001b[39m=\u001b[39m [\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bsshd.jarvislabs.ai/home/src/train-model-gal.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=116'>117</a>\u001b[0m             [\u001b[39m\"\u001b[39m\u001b[39mBowel\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bsshd.jarvislabs.ai/home/src/train-model-gal.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=117'>118</a>\u001b[0m                 train_acc_bowel\u001b[39m.\u001b[39mcompute_accuracy(),\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bsshd.jarvislabs.ai/home/src/train-model-gal.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=118'>119</a>\u001b[0m                 val_acc_bowel\u001b[39m.\u001b[39mcompute_accuracy(),\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bsshd.jarvislabs.ai/home/src/train-model-gal.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=119'>120</a>\u001b[0m                 train_acc_bowel\u001b[39m.\u001b[39;49mcompute_auc(),\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bsshd.jarvislabs.ai/home/src/train-model-gal.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=120'>121</a>\u001b[0m                 val_acc_bowel\u001b[39m.\u001b[39mcompute_auc()],\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bsshd.jarvislabs.ai/home/src/train-model-gal.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=121'>122</a>\u001b[0m             [\u001b[39m\"\u001b[39m\u001b[39mExtravasation\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bsshd.jarvislabs.ai/home/src/train-model-gal.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=122'>123</a>\u001b[0m                 train_acc_extravasation\u001b[39m.\u001b[39mcompute_accuracy(),\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bsshd.jarvislabs.ai/home/src/train-model-gal.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=123'>124</a>\u001b[0m                 val_acc_extravasation\u001b[39m.\u001b[39mcompute_accuracy(),\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bsshd.jarvislabs.ai/home/src/train-model-gal.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=124'>125</a>\u001b[0m                 train_acc_extravasation\u001b[39m.\u001b[39mcompute_auc(),\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bsshd.jarvislabs.ai/home/src/train-model-gal.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=125'>126</a>\u001b[0m                 val_acc_extravasation\u001b[39m.\u001b[39mcompute_auc()],\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bsshd.jarvislabs.ai/home/src/train-model-gal.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=126'>127</a>\u001b[0m             [\u001b[39m\"\u001b[39m\u001b[39mLiver\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bsshd.jarvislabs.ai/home/src/train-model-gal.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=127'>128</a>\u001b[0m                 train_acc_liver\u001b[39m.\u001b[39mcompute_accuracy(),\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bsshd.jarvislabs.ai/home/src/train-model-gal.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=128'>129</a>\u001b[0m                 val_acc_liver\u001b[39m.\u001b[39mcompute_accuracy(),\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bsshd.jarvislabs.ai/home/src/train-model-gal.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=129'>130</a>\u001b[0m                 train_acc_liver\u001b[39m.\u001b[39mcompute_auc(),\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bsshd.jarvislabs.ai/home/src/train-model-gal.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=130'>131</a>\u001b[0m                 val_acc_liver\u001b[39m.\u001b[39mcompute_auc()],\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bsshd.jarvislabs.ai/home/src/train-model-gal.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=131'>132</a>\u001b[0m             [\u001b[39m\"\u001b[39m\u001b[39mKidney\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bsshd.jarvislabs.ai/home/src/train-model-gal.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=132'>133</a>\u001b[0m                 train_acc_kidney\u001b[39m.\u001b[39mcompute_accuracy(),\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bsshd.jarvislabs.ai/home/src/train-model-gal.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=133'>134</a>\u001b[0m                 val_acc_kidney\u001b[39m.\u001b[39mcompute_accuracy(),\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bsshd.jarvislabs.ai/home/src/train-model-gal.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=134'>135</a>\u001b[0m                 train_acc_kidney\u001b[39m.\u001b[39mcompute_auc(),\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bsshd.jarvislabs.ai/home/src/train-model-gal.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=135'>136</a>\u001b[0m                 val_acc_kidney\u001b[39m.\u001b[39mcompute_auc()],\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bsshd.jarvislabs.ai/home/src/train-model-gal.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=136'>137</a>\u001b[0m             [\u001b[39m\"\u001b[39m\u001b[39mSpleen\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bsshd.jarvislabs.ai/home/src/train-model-gal.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=137'>138</a>\u001b[0m                 train_acc_spleen\u001b[39m.\u001b[39mcompute_accuracy(),\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bsshd.jarvislabs.ai/home/src/train-model-gal.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=138'>139</a>\u001b[0m                 val_acc_spleen\u001b[39m.\u001b[39mcompute_accuracy(),\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bsshd.jarvislabs.ai/home/src/train-model-gal.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=139'>140</a>\u001b[0m                 train_acc_spleen\u001b[39m.\u001b[39mcompute_auc(),\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bsshd.jarvislabs.ai/home/src/train-model-gal.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=140'>141</a>\u001b[0m                 val_acc_spleen\u001b[39m.\u001b[39mcompute_auc()]\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bsshd.jarvislabs.ai/home/src/train-model-gal.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=141'>142</a>\u001b[0m         ]\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bsshd.jarvislabs.ai/home/src/train-model-gal.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=142'>143</a>\u001b[0m \u001b[39m# verbose\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bsshd.jarvislabs.ai/home/src/train-model-gal.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=143'>144</a>\u001b[0m \u001b[39mprint\u001b[39m(tabulate(metrics_data, headers\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mTrain Acc\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mVal Acc\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mTrain AUC\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mVal AUC\u001b[39m\u001b[39m\"\u001b[39m]))\n",
      "\u001b[1;32m/home/src/train-model-gal.ipynb Cell 20\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsshd.jarvislabs.ai/home/src/train-model-gal.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m roc_auc_score(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtargets, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprobabilities, multi_class \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39movo\u001b[39m\u001b[39m'\u001b[39m, labels\u001b[39m=\u001b[39m[\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsshd.jarvislabs.ai/home/src/train-model-gal.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bsshd.jarvislabs.ai/home/src/train-model-gal.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtargets\u001b[39m.\u001b[39;49mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsshd.jarvislabs.ai/home/src/train-model-gal.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprobabilities\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsshd.jarvislabs.ai/home/src/train-model-gal.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m roc_auc_score(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtargets, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprobabilities)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "#main training function \n",
    "def main_function():\n",
    "\n",
    "    model = CT3DModel(num_classes=13).to(device)\n",
    "    #define loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5, verbose=True)\n",
    "    loss_two_bowl = torch.nn.CrossEntropyLoss(weight = torch.tensor([1.0, 2.0])).to(device)\n",
    "    loss_two_extra = torch.nn.CrossEntropyLoss(weight = torch.tensor([1.0, 6.0])).to(device)\n",
    "    loss_three  = torch.nn.CrossEntropyLoss(label_smoothing = 0.05, weight = torch.tensor([1.0, 2.0, 4.0])).to(device)\n",
    "    \n",
    "    train_losses_ephocs = []\n",
    "    val_losses_ephocs = []\n",
    "    best_loss=np.inf\n",
    "    \n",
    "    for epoch in range(Config.EPOCHS):\n",
    "        train_lose_ephoc =0\n",
    "        val_lose_ephoc = 0\n",
    "        train_loss=0\n",
    "        val_loss=0\n",
    "        print(f'Epoch: [{epoch+1}/{Config.EPOCHS}]')\n",
    "        print(f'Fold: {epoch%5}')\n",
    "        train_dataloader  = train_dataloaders[epoch%5]\n",
    "        val_dataloader    = val_dataloaders[epoch%5]               \n",
    "    \n",
    "        for batch_idx, (images, labels,patient_id) in enumerate(train_dataloader):\n",
    "            #################################### strat train ephoch  #########################################\n",
    "            \n",
    "            print(len(train_dataloader))\n",
    "            print(batch_idx)\n",
    "            optimizer.zero_grad()\n",
    "            # Move data to GPU           \n",
    "            images = images.to(device)  \n",
    "            concatenated_categorial=convertOneHotToCategorial(labels).to(device)  \n",
    "            bowel_batch_labels,extravasation_batch_labels=concatenated_categorial[:,0],concatenated_categorial[:,1]\n",
    "            kidney_batch_labels,liver_batch_labels,spleen_batch_labels=concatenated_categorial[:,2],concatenated_categorial[:,3],concatenated_categorial[:,4]\n",
    "\n",
    "            ## debug  ### \n",
    "            #----plot 10 fist scans\n",
    "            # debug_plot_images(images,10,patient_id)\n",
    "            # print(images.shape)\n",
    "            # print(labels)\n",
    "                 \n",
    "            \n",
    "            outputs = model(images)\n",
    "            print(outputs)\n",
    "            print(outputs[:, 0:2].dtype)  # Should be torch.float32 or similar      \n",
    "            bowel_loss = loss_two_bowl(outputs[:, 0:2], bowel_batch_labels)\n",
    "            extravasation_loss = loss_two_extra(outputs[:, 2:4], extravasation_batch_labels)\n",
    "            kidney_loss = loss_three(outputs[:, 4:7], kidney_batch_labels)\n",
    "            liver_loss = loss_three(outputs[:, 7:10], liver_batch_labels)\n",
    "            spleen_loss = loss_three(outputs[:, 10:13], spleen_batch_labels)\n",
    "            ###!!!!!!!!!To-do -add anyinjuery loss!!!!!!!!!!!!!!!\n",
    "            train_loss = bowel_loss + extravasation_loss + kidney_loss + liver_loss + spleen_loss\n",
    "            print(train_loss)\n",
    "            # Backward pass\n",
    "            \n",
    "            train_loss.backward()\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            #aggregate results for metrics\n",
    "            train_lose_ephoc += train_loss.item()\n",
    "            train_acc_bowel.update(outputs[:, 0:2], bowel_batch_labels)\n",
    "            train_acc_extravasation.update(outputs[:, 2:4], extravasation_batch_labels)\n",
    "            train_acc_kidney.update(outputs[:, 4:7], kidney_batch_labels)\n",
    "            train_acc_liver.update(outputs[:, 7:10], liver_batch_labels)\n",
    "            train_acc_spleen.update(outputs[:, 10:13], spleen_batch_labels)\n",
    "            \n",
    "        ############################################end train ephoch  ################################## \n",
    "        train_lose_ephoc=train_lose_ephoc/len(train_dataloader)\n",
    "        train_losses_ephocs.append( train_lose_ephoc)\n",
    "          \n",
    "        model.eval()  # Set the model to evaluation mode    \n",
    "        with torch.no_grad():\n",
    "            for val_ind, (images, labels,patient_id) in enumerate(val_dataloader):  \n",
    "                ##########################  strat eval  ephoch   #############################\n",
    "                \n",
    "                images = images.to(device)   \n",
    "                #parse labels\n",
    "                concatenated_categorial=convertOneHotToCategorial(labels).to(device)\n",
    "                bowel_batch_labels,extravasation_batch_labels=concatenated_categorial[:,0],concatenated_categorial[:,1]\n",
    "                kidney_batch_labels,liver_batch_labels,spleen_batch_labels=concatenated_categorial[:,2],concatenated_categorial[:,3],concatenated_categorial[:,4]\n",
    "           \n",
    "                outputs = model(images)\n",
    "                bowel_loss = loss_two_bowl(outputs[:, 0:2], bowel_batch_labels)\n",
    "                extravasation_loss = loss_two_extra(outputs[:, 2:4], extravasation_batch_labels)\n",
    "                kidney_loss = loss_three(outputs[:, 4:7], kidney_batch_labels)\n",
    "                liver_loss = loss_three(outputs[:, 7:10], liver_batch_labels)\n",
    "                spleen_loss = loss_three(outputs[:, 10:13], spleen_batch_labels)\n",
    "                \n",
    "                ###!!!!!!!!!To-do -add any injuery loss!!!!!!!!!!!!!!!\n",
    "                \n",
    "                val_loss = bowel_loss + extravasation_loss + kidney_loss + liver_loss + spleen_loss \n",
    "                # calculate validation metrics    \n",
    "                val_lose_ephoc += val_loss.item()\n",
    "                val_acc_bowel.update(outputs[:, 0:2], bowel_batch_labels)\n",
    "                val_acc_extravasation.update(outputs[:, 2:4], extravasation_batch_labels)\n",
    "                val_acc_kidney.update(outputs[:, 4:7], kidney_batch_labels)\n",
    "                val_acc_liver.update(outputs[:, 7:10], liver_batch_labels)\n",
    "                val_acc_spleen.update(outputs[:, 10:13], spleen_batch_labels)\n",
    "                \n",
    "                ################################# end eval ephoch #############################\n",
    "                \n",
    "        # to get out of plauto         \n",
    "        val_lose_ephoc=val_lose_ephoc/len(val_dataloader)\n",
    "        scheduler.step(val_lose_ephoc)      \n",
    "        val_losses_ephocs.append(val_lose_ephoc)     \n",
    "        if val_loss <= best_loss:\n",
    "          best_loss = val_loss\n",
    "        #   torch.save(model.state_dict(), './drive/MyDrive/model_weights.pth')\n",
    "        print(f\"Epoch [{epoch+1}/{Config.EPOCHS}] - train Loss: {train_lose_ephoc:.4f} - Val Loss: {val_lose_ephoc:.4f}\")\n",
    "        # accuracy and auc data\n",
    "        print(train_acc_bowel.targets)\n",
    "        print(train_acc_bowel.predictions)\n",
    "        print(train_acc_bowel.probabilities)\n",
    "        metrics_data = [\n",
    "                    [\"Bowel\", \n",
    "                        train_acc_bowel.compute_accuracy(),\n",
    "                        val_acc_bowel.compute_accuracy(),\n",
    "                        train_acc_bowel.compute_auc(),\n",
    "                        val_acc_bowel.compute_auc()],\n",
    "                    [\"Extravasation\", \n",
    "                        train_acc_extravasation.compute_accuracy(),\n",
    "                        val_acc_extravasation.compute_accuracy(),\n",
    "                        train_acc_extravasation.compute_auc(),\n",
    "                        val_acc_extravasation.compute_auc()],\n",
    "                    [\"Liver\", \n",
    "                        train_acc_liver.compute_accuracy(),\n",
    "                        val_acc_liver.compute_accuracy(),\n",
    "                        train_acc_liver.compute_auc(),\n",
    "                        val_acc_liver.compute_auc()],\n",
    "                    [\"Kidney\", \n",
    "                        train_acc_kidney.compute_accuracy(),\n",
    "                        val_acc_kidney.compute_accuracy(),\n",
    "                        train_acc_kidney.compute_auc(),\n",
    "                        val_acc_kidney.compute_auc()],\n",
    "                    [\"Spleen\", \n",
    "                        train_acc_spleen.compute_accuracy(),\n",
    "                        val_acc_spleen.compute_accuracy(),\n",
    "                        train_acc_spleen.compute_auc(),\n",
    "                        val_acc_spleen.compute_auc()]\n",
    "                ]\n",
    "        # verbose\n",
    "        print(tabulate(metrics_data, headers=[\"\", \"Train Acc\", \"Val Acc\", \"Train AUC\", \"Val AUC\"]))\n",
    "        #reset metrics\n",
    "        train_acc_bowel.reset()\n",
    "        train_acc_extravasation.reset()\n",
    "        train_acc_liver.reset()\n",
    "        train_acc_kidney.reset()\n",
    "        train_acc_spleen.reset()\n",
    "        val_acc_bowel.reset()\n",
    "        val_acc_extravasation.reset()\n",
    "        val_acc_liver.reset()\n",
    "        val_acc_kidney.reset()\n",
    "        val_acc_spleen.reset()     \n",
    "        \n",
    "def convertOneHotToCategorial(labels):\n",
    "        bowel_batch_labels = np.argmax(labels[:,0:2],axis=1 ,keepdims = True).squeeze(-1)\n",
    "        extravasation_batch_labels = np.argmax(labels[:,2:4],axis=1 ,keepdims = True).squeeze(-1)\n",
    "        kidney_batch_labels = np.argmax(labels[:,4:7],axis=1 ,keepdims = True).squeeze(-1)\n",
    "        liver_batch_labels = np.argmax(labels[:,7:10],axis=1, keepdims = True).squeeze(-1)\n",
    "        spleen_batch_labels = np.argmax(labels[:,10:],axis=1, keepdims = True).squeeze(-1)\n",
    "        concatenated_categorial =np.vstack((bowel_batch_labels, extravasation_batch_labels, kidney_batch_labels,liver_batch_labels,spleen_batch_labels)).T\n",
    "        concatenated_categorial = torch.tensor(concatenated_categorial)\n",
    "        return concatenated_categorial\n",
    "\n",
    "def debug_plot_images(images,num_scans,patient_id):\n",
    "    \n",
    "    first_scans = images[:num_scans]\n",
    "            # Plot the firsts scans\n",
    "    for scan_idx, scan in enumerate(first_scans):\n",
    "                print(f'patient id {patient_id[scan_idx]}')\n",
    "                num_images = scan.shape[0]\n",
    "                plt.figure(figsize=(20, 20))  # Adjust the figure size as needed\n",
    "                plt.suptitle(f\"Scan {scan_idx + 1}\")\n",
    "                for image_idx in range(num_images):\n",
    "                    plt.subplot(30, 10, image_idx + 1)\n",
    "                    plt.imshow(scan[image_idx], cmap='gray')  # Assuming grayscale images\n",
    "                    plt.axis('off')  # Turn off axis labels\n",
    "\n",
    "                plt.show()\n",
    "    \n",
    "main_function()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './drive/MyDrive/model_ResNet18+3DConv_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_window = vtk.vtkRenderWindow()\n",
    "render_window_interactor = vtk.vtkRenderWindowInteractor()\n",
    "render_window_interactor.SetRenderWindow(render_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation progress\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses_ephocs, label='Train')\n",
    "plt.plot(val_losses_ephocs, label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load('model.pth')\n",
    "# model.load_state_dict(torch.load('./drive/MyDrive/DATA/model_weights.pth'))\n",
    "\n",
    "# trt_model = torch_tensorrt.compile(model, \n",
    "#     inputs= [torch_tensorrt.Input((1, 3, 224, 224))],\n",
    "#     enabled_precisions= { torch_tensorrt.dtype.half} # Run with FP16\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
